{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5LE4lMvd1j11ya8c/p/Vi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RithwikMohan/ML_Lab/blob/main/perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.)Simple Perceptron"
      ],
      "metadata": {
        "id": "wEMr3xn6gJFG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUHrYcKrgEys",
        "outputId": "4a48b5bd-e1e8-4366-fc1a-c12cb9a1810e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neuron output: 0.9990889488055994\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "class Neuron:\n",
        "    def __init__(self, weights, bias):\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "\n",
        "    def feedforward(self, inputs):\n",
        "        total = np.dot(self.weights, inputs) + self.bias\n",
        "        return sigmoid(total)\n",
        "\n",
        "# Example\n",
        "weights = np.array([0, 1])  # w1=0, w2=1\n",
        "bias = 4\n",
        "n = Neuron(weights, bias)\n",
        "\n",
        "x = np.array([2, 3])  # input\n",
        "print(\"Neuron output:\", n.feedforward(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.)Perceptron with activation Function with AND, OR, XOR"
      ],
      "metadata": {
        "id": "QxF-n88ggVo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.1, epochs=10):\n",
        "        self.learning_rate = learning_rate#learning_rate: how much to adjust weights when an error occurs,epochs: how many times to iterate over the entire dataset.\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def step_function(self, x):\n",
        "        return np.where(x >= 0, 1, 0)#If the weighted sum ≥ 0 ⇒ output = 1 else 0\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            for i in range(n_samples):\n",
        "                linear_output = np.dot(X[i], self.weights) + self.bias#computing w.x+b and then applying step fn to get 0/1\n",
        "                y_pred = self.step_function(linear_output)\n",
        "                update = self.learning_rate * (y[i] - y_pred)\n",
        "                self.weights += update * X[i]\n",
        "                self.bias += update\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        return self.step_function(linear_output)\n",
        "\n",
        "\n",
        "#AND Gate\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y_and = np.array([0,0,0,1])\n",
        "p_and = Perceptron()\n",
        "p_and.fit(X, y_and)\n",
        "print(\"AND Predictions:\", p_and.predict(X))\n",
        "\n",
        "#OR Gate\n",
        "y_or = np.array([0,1,1,1])\n",
        "p_or = Perceptron()\n",
        "p_or.fit(X, y_or)\n",
        "print(\"OR Predictions:\", p_or.predict(X))\n",
        "\n",
        "#XOR Gate (not linearly separable, will fail intentionally)no single straight line can divide 1s and 0s we need at least two layers (an MLP)\n",
        "y_xor = np.array([0,1,1,0])\n",
        "p_xor = Perceptron()\n",
        "p_xor.fit(X, y_xor)\n",
        "print(\"XOR Predictions:\", p_xor.predict(X))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6UFIOlogVJ9",
        "outputId": "d21ee68e-b7f7-4fa0-e536-0497a778d76c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AND Predictions: [0 0 0 1]\n",
            "OR Predictions: [0 1 1 1]\n",
            "XOR Predictions: [1 1 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.)MLP (Multi-Layer Perceptron) with Single Hidden Layer Given: weights = [0, 1] bias = 0 x = [2, 3] sigmoid(z) = 1 / (1 + exp(-z))\n",
        "\n",
        "Step 1: hidden neuron h1 total_h1 = 02 + 13 + 0 = 3 out_h1 = sigmoid(3) = 1 / (1 + e^(-3)) = 0.9525741268224334\n",
        "\n",
        "Step 2: hidden neuron h2 (same weights) total_h2 = 3 out_h2 = sigmoid(3) = 0.9525741268224334\n",
        "\n",
        "Step 3: output neuron o1 (inputs = [out_h1, out_h2]) total_o1 = 0out_h1 + 1out_h2 + 0 = 0.9525741268224334 out_o1 = sigmoid(total_o1) = sigmoid(0.9525741268224334) = 1 / (1 + e^(-0.9525741268224334)) = 0.7216325609518421\n",
        "\n",
        "Final network output: 0.7216325609518421\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UuHjNSZJgat9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "class Neuron:\n",
        "    def __init__(self, weights, bias):\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "\n",
        "    def feedforward(self, inputs):\n",
        "        total = np.dot(self.weights, inputs) + self.bias\n",
        "        return sigmoid(total)\n",
        "\n",
        "class OurNeuralNetwork:\n",
        "    \"\"\"\n",
        "    Neural Network with:\n",
        "    - 2 input neurons\n",
        "    - 2 hidden neurons (h1, h2)\n",
        "    - 1 output neuron (o1)\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        weights = np.array([0, 1])\n",
        "        bias = 0\n",
        "        self.h1 = Neuron(weights, bias)\n",
        "        self.h2 = Neuron(weights, bias)\n",
        "        self.o1 = Neuron(weights, bias)\n",
        "\n",
        "    def feedforward(self, x):\n",
        "        out_h1 = self.h1.feedforward(x)\n",
        "        out_h2 = self.h2.feedforward(x)\n",
        "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
        "        return out_o1\n",
        "\n",
        "# Example\n",
        "network = OurNeuralNetwork()\n",
        "x = np.array([2, 3])\n",
        "print(\"Network output:\", network.feedforward(x))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BD7HSp1dgaVO",
        "outputId": "ad132269-6144-475f-e590-334308d024bb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network output: 0.7216325609518421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MNIST MLP(Multi-Layer perceptron) Classifier"
      ],
      "metadata": {
        "id": "78u6ToZuggY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml                # To load the MNIST dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "# fetch_openml downloads the dataset from OpenML\n",
        "# 'mnist_784' → dataset name (contains 70,000 images of digits, each 28x28 = 784 pixels)\n",
        "# version=1 → dataset version\n",
        "# return_X_y=True → directly returns features (X) and labels (y) separately\n",
        "data, labels = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "\n",
        "# Each pixel value ranges from 0 to 255 — normalizing them to 0–1 for faster training\n",
        "data = data / 255.0\n",
        "# Split the dataset into training and testing sets\n",
        "# random_state=42 → ensures reproducibility (same random split each run)\n",
        "# stratify=labels → ensures equal class distribution in both train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data, labels, test_size=0.1, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "# Initializing the Multi-Layer Perceptron (MLP)\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(50,),   # One hidden layer with 50 neurons\n",
        "    max_iter=50,                # Maximum 50 training iterations (epochs)\n",
        "    verbose=True,               # Prints training progress and loss values\n",
        "    random_state=1              # For reproducibility of weight initialization\n",
        ")\n",
        "# Train the model (Backpropagation)\n",
        "# .fit() automatically performs:\n",
        "#   - Forward pass → calculates outputs\n",
        "#   - Backward pass → adjusts weights using gradient descent\n",
        "#   - Repeats for 'max_iter' epochs\n",
        "mlp.fit(X_train, y_train)\n",
        "# .predict() → predicts labels for test set\n",
        "y_pred = mlp.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvTbXIh0gg-Q",
        "outputId": "f6f2274e-d771-43b6-f7e2-b7db0f71348a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.50240991\n",
            "Iteration 2, loss = 0.24293823\n",
            "Iteration 3, loss = 0.19396647\n",
            "Iteration 4, loss = 0.16213879\n",
            "Iteration 5, loss = 0.14071148\n",
            "Iteration 6, loss = 0.12341883\n",
            "Iteration 7, loss = 0.10922166\n",
            "Iteration 8, loss = 0.09774355\n",
            "Iteration 9, loss = 0.08919639\n",
            "Iteration 10, loss = 0.08139225\n",
            "Iteration 11, loss = 0.07540877\n",
            "Iteration 12, loss = 0.06954664\n",
            "Iteration 13, loss = 0.06361726\n",
            "Iteration 14, loss = 0.05979103\n",
            "Iteration 15, loss = 0.05585580\n",
            "Iteration 16, loss = 0.05202626\n",
            "Iteration 17, loss = 0.04899432\n",
            "Iteration 18, loss = 0.04561763\n",
            "Iteration 19, loss = 0.04281492\n",
            "Iteration 20, loss = 0.04050308\n",
            "Iteration 21, loss = 0.03787670\n",
            "Iteration 22, loss = 0.03558628\n",
            "Iteration 23, loss = 0.03292704\n",
            "Iteration 24, loss = 0.03132445\n",
            "Iteration 25, loss = 0.02988745\n",
            "Iteration 26, loss = 0.02857582\n",
            "Iteration 27, loss = 0.02623057\n",
            "Iteration 28, loss = 0.02454969\n",
            "Iteration 29, loss = 0.02335991\n",
            "Iteration 30, loss = 0.02190347\n",
            "Iteration 31, loss = 0.02054652\n",
            "Iteration 32, loss = 0.01955444\n",
            "Iteration 33, loss = 0.01806062\n",
            "Iteration 34, loss = 0.01723213\n",
            "Iteration 35, loss = 0.01601015\n",
            "Iteration 36, loss = 0.01543006\n",
            "Iteration 37, loss = 0.01425774\n",
            "Iteration 38, loss = 0.01361097\n",
            "Iteration 39, loss = 0.01272899\n",
            "Iteration 40, loss = 0.01192553\n",
            "Iteration 41, loss = 0.01160828\n",
            "Iteration 42, loss = 0.01075935\n",
            "Iteration 43, loss = 0.00983147\n",
            "Iteration 44, loss = 0.00952984\n",
            "Iteration 45, loss = 0.00876814\n",
            "Iteration 46, loss = 0.00864342\n",
            "Iteration 47, loss = 0.00796637\n",
            "Iteration 48, loss = 0.00720160\n",
            "Iteration 49, loss = 0.00758951\n",
            "Iteration 50, loss = 0.00681590\n",
            "Test Accuracy: 0.97\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}